{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6537093",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3bf899",
   "metadata": {},
   "source": [
    "In this notebook, we will look at some different regression techniques using the Hitters dataset - we attempt to predict the salary of a baseball player from their performance on the field!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b4d068",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mattplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmattplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mattplotlib'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import mattplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing, dropping NaNs, removing name variables, one-hot-encoding, etc.\n",
    "df = pd.read_csv('Hitters.csv')\n",
    "df = pd.read_csv('Hitters.csv').dropna().drop(df.columns[0], axis = 1)\n",
    "dummies = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\n",
    "y = df.Salary\n",
    "X_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis = 1).astype('float64')\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis = 1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c2da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X) #scaling the data, otherwise Ridge and Lasso won't work\n",
    "X_scaled = scaler.transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16516a97",
   "metadata": {},
   "source": [
    "## Ordinary least squares regression (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0555dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "print(\"r^2 on train data is {}\".format(ols.score(X_train, y_train)))\n",
    "print(\"r^2 on test data is {}\".format(ols.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7160e1",
   "metadata": {},
   "source": [
    "The model is not fantastic and also there's some overfitting going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d108ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intercept: {}\".format(ols.intercept_))\n",
    "\n",
    "n_features = len(ols.coef_)\n",
    "plt.figure(dpi = 800)\n",
    "plt.barh(range(n_features), ols.coef_, align='center')\n",
    "plt.yticks(np.arange(n_features), X.columns)\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2f108",
   "metadata": {},
   "source": [
    "This figure allows us to extract the most important features for salary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776283f2",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6700e05",
   "metadata": {},
   "source": [
    "We try to find a good value for alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e60297",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "best_ridge, best_ridge_alpha = None, None\n",
    "best_ridge_mse = float(\"inf\")\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge_model = Ridge(alpha=alpha, max_iter=100000)\n",
    "    ridge_model.fit(X_train, y_train)  \n",
    "    mse = mean_squared_error(y_test, ridge_model.predict(X_test))\n",
    "    \n",
    "    if mse < best_ridge_mse:\n",
    "        best_ridge_mse = mse\n",
    "        best_ridge = ridge_model\n",
    "        best_ridge_alpha = alpha\n",
    "\n",
    "print(best_ridge_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cdfb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha = best_ridge_alpha)\n",
    "ridge.fit(X_train, y_train)\n",
    "print(\"r^2 on train data is {}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"r^2 on test data is {}\".format(ridge.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7d1a19",
   "metadata": {},
   "source": [
    "It's not a huge improvement, but it is indeed an improvement and there's a little less overfitting than OLS. Increasing alpha will decrease r^2 a bit, but get the two closer to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffcc6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intercept: {}\".format(ridge.intercept_))\n",
    "\n",
    "n_features = len(ridge.coef_)\n",
    "plt.figure(dpi = 800)\n",
    "plt.barh(range(n_features), ridge.coef_, align='center')\n",
    "plt.yticks(np.arange(n_features), X.columns)\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3a1cdd",
   "metadata": {},
   "source": [
    "Notice that the coefficients are much smaller than for OLS - that's what Ridge regression can do!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf14cf74",
   "metadata": {},
   "source": [
    "## Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7266ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "best_lasso, best_lasso_alpha = None, None\n",
    "best_lasso_mse = float(\"inf\")\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso_model = Lasso(alpha=alpha, max_iter=100000)\n",
    "    lasso_model.fit(X_train, y_train)  \n",
    "    mse = mean_squared_error(y_test, lasso_model.predict(X_test))\n",
    "    \n",
    "    if mse < best_lasso_mse:\n",
    "        best_lasso_mse = mse\n",
    "        best_lasso = lasso_model\n",
    "        best_lasso_alpha = alpha\n",
    "\n",
    "print(best_lasso_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha = best_lasso_alpha)\n",
    "lasso.fit(X_train, y_train)\n",
    "print(\"r^2 on train data is {}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"r^2 on test data is {}\".format(lasso.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eebaaa",
   "metadata": {},
   "source": [
    "A similar r^2 to Ridge for the test data, but the two values are a bit closer to each other - slightly less overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9faa5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intercept: {}\".format(lasso.intercept_))\n",
    "\n",
    "n_features = len(lasso.coef_)\n",
    "plt.figure(dpi = 800)\n",
    "plt.barh(range(n_features), lasso.coef_, align='center')\n",
    "plt.yticks(np.arange(n_features), X.columns)\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab404f",
   "metadata": {},
   "source": [
    "Behold the power of Lasso regression: Several coefficients are exactly zero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d837e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=800)\n",
    "plt.plot(y_test, lasso.predict(X_test),'.')\n",
    "plt.plot([0,1200], [0, 1200],'-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8002813a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
